#!/bin/bash\n\n# Production Analytics Deployment Script\n# Phase 4 Advanced Analytics - Production Deployment\n#\n# Features:\n# - Automated deployment with health checks\n# - Service orchestration and configuration\n# - Database initialization and migrations\n# - Monitoring and alerting setup\n# - Zero-downtime deployment support\n\nset -euo pipefail\n\n# Script configuration\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPROJECT_ROOT=\"$(cd \"$SCRIPT_DIR/..\" && pwd)\"\nLOG_FILE=\"$PROJECT_ROOT/logs/deploy-$(date +%Y%m%d-%H%M%S).log\"\nCONFIG_DIR=\"$PROJECT_ROOT/config\"\nSERVICES_DIR=\"$PROJECT_ROOT\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nPURPLE='\\033[0;35m'\nCYAN='\\033[0;36m'\nNC='\\033[0m' # No Color\n\n# Logging function\nlog() {\n    local level=\"$1\"\n    shift\n    local message=\"$*\"\n    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')\n    echo \"[$timestamp] [$level] $message\" | tee -a \"$LOG_FILE\"\n}\n\ninfo() { echo -e \"${BLUE}ℹ️  $*${NC}\"; log \"INFO\" \"$*\"; }\nwarn() { echo -e \"${YELLOW}⚠️  $*${NC}\"; log \"WARN\" \"$*\"; }\nerror() { echo -e \"${RED}❌ $*${NC}\"; log \"ERROR\" \"$*\"; }\nsuccess() { echo -e \"${GREEN}✅ $*${NC}\"; log \"SUCCESS\" \"$*\"; }\n\n# Environment detection\ndetect_environment() {\n    if [[ \"${ENVIRONMENT:-}\" ]]; then\n        echo \"$ENVIRONMENT\"\n    elif [[ -f \"$PROJECT_ROOT/.env\" ]]; then\n        grep '^ENVIRONMENT=' \"$PROJECT_ROOT/.env\" | cut -d'=' -f2 | tr -d '\"'\n    else\n        echo \"development\"\n    fi\n}\n\nENV=$(detect_environment)\ninfo \"Detected environment: $ENV\"\n\n# Configuration loading\nload_config() {\n    local config_file=\"$CONFIG_DIR/deploy-$ENV.toml\"\n    if [[ -f \"$config_file\" ]]; then\n        info \"Loading configuration from $config_file\"\n        # In a real implementation, you'd parse TOML here\n        # For now, we'll use environment variables\n    else\n        warn \"Configuration file $config_file not found, using defaults\"\n    fi\n}\n\n# Prerequisites check\ncheck_prerequisites() {\n    info \"Checking deployment prerequisites...\"\n    \n    local missing_tools=()\n    \n    # Check required tools\n    local required_tools=(\"docker\" \"docker-compose\" \"python3\" \"pip\" \"cargo\" \"rustc\")\n    for tool in \"${required_tools[@]}\"; do\n        if ! command -v \"$tool\" &> /dev/null; then\n            missing_tools+=(\"$tool\")\n        fi\n    done\n    \n    if [[ ${#missing_tools[@]} -gt 0 ]]; then\n        error \"Missing required tools: ${missing_tools[*]}\"\n        exit 1\n    fi\n    \n    # Check Docker daemon\n    if ! docker info &> /dev/null; then\n        error \"Docker daemon is not running\"\n        exit 1\n    fi\n    \n    # Check disk space (minimum 10GB)\n    local available_space=$(df \"$PROJECT_ROOT\" | awk 'NR==2 {print $4}')\n    local min_space=10485760  # 10GB in KB\n    if [[ $available_space -lt $min_space ]]; then\n        error \"Insufficient disk space. Available: $(($available_space/1024/1024))GB, Required: 10GB\"\n        exit 1\n    fi\n    \n    # Check memory (minimum 8GB)\n    local total_memory=$(free -k | awk '/^Mem:/{print $2}')\n    local min_memory=8388608  # 8GB in KB\n    if [[ $total_memory -lt $min_memory ]]; then\n        warn \"Low memory detected. Available: $(($total_memory/1024/1024))GB, Recommended: 8GB\"\n    fi\n    \n    success \"Prerequisites check passed\"\n}\n\n# Database setup\nsetup_database() {\n    info \"Setting up analytics databases...\"\n    \n    # Create necessary directories\n    mkdir -p \"$PROJECT_ROOT/data/postgresql\"\n    mkdir -p \"$PROJECT_ROOT/data/redis\"\n    mkdir -p \"$PROJECT_ROOT/logs/database\"\n    \n    # Start database services\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" up -d postgres redis\n    \n    # Wait for PostgreSQL to be ready\n    info \"Waiting for PostgreSQL to be ready...\"\n    local retries=30\n    while [[ $retries -gt 0 ]]; do\n        if docker exec analytics_postgres pg_isready -U analytics &> /dev/null; then\n            break\n        fi\n        sleep 2\n        ((retries--))\n    done\n    \n    if [[ $retries -eq 0 ]]; then\n        error \"PostgreSQL failed to start within timeout\"\n        exit 1\n    fi\n    \n    # Run database migrations\n    info \"Running database migrations...\"\n    docker exec analytics_postgres psql -U analytics -d analytics -f /migrations/001_initial_schema.sql\n    docker exec analytics_postgres psql -U analytics -d analytics -f /migrations/002_analytics_tables.sql\n    docker exec analytics_postgres psql -U analytics -d analytics -f /migrations/003_retention_tables.sql\n    \n    success \"Database setup completed\"\n}\n\n# Build services\nbuild_services() {\n    info \"Building analytics services...\"\n    \n    # Build Rust components\n    info \"Building Rust ML insights engine...\"\n    cd \"$PROJECT_ROOT/analytics\"\n    cargo build --release --bin ml-insights-engine\n    \n    info \"Building Rust SLA monitor...\"\n    cd \"$PROJECT_ROOT/reporting\"\n    cargo build --release --bin sla-monitor\n    \n    info \"Building Rust API endpoints...\"\n    cd \"$PROJECT_ROOT/analytics\"\n    cargo build --release --bin api-endpoints\n    \n    # Install Python dependencies\n    info \"Installing Python dependencies...\"\n    cd \"$PROJECT_ROOT\"\n    python3 -m venv venv\n    source venv/bin/activate\n    pip install --upgrade pip\n    pip install -r requirements.txt\n    \n    # Build Docker images\n    info \"Building Docker images...\"\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" build\n    \n    success \"Service build completed\"\n}\n\n# Deploy configuration\ndeploy_configuration() {\n    info \"Deploying configuration files...\"\n    \n    # Copy configuration files\n    cp \"$CONFIG_DIR/data-retention.toml\" \"$PROJECT_ROOT/config/\"\n    \n    # Generate environment-specific configs\n    if [[ \"$ENV\" == \"production\" ]]; then\n        # Production-specific configurations\n        export ANALYTICS_LOG_LEVEL=\"INFO\"\n        export ANALYTICS_WORKERS=\"8\"\n        export ANALYTICS_MAX_CONNECTIONS=\"100\"\n        export ANALYTICS_RATE_LIMIT=\"10000\"\n    elif [[ \"$ENV\" == \"staging\" ]]; then\n        # Staging-specific configurations\n        export ANALYTICS_LOG_LEVEL=\"DEBUG\"\n        export ANALYTICS_WORKERS=\"4\"\n        export ANALYTICS_MAX_CONNECTIONS=\"50\"\n        export ANALYTICS_RATE_LIMIT=\"5000\"\n    else\n        # Development configurations\n        export ANALYTICS_LOG_LEVEL=\"DEBUG\"\n        export ANALYTICS_WORKERS=\"2\"\n        export ANALYTICS_MAX_CONNECTIONS=\"20\"\n        export ANALYTICS_RATE_LIMIT=\"1000\"\n    fi\n    \n    # Create environment file\n    cat > \"$PROJECT_ROOT/.env\" << EOF\n# Analytics Environment Configuration\nENVIRONMENT=$ENV\nANALYTICS_LOG_LEVEL=${ANALYTICS_LOG_LEVEL}\nANALYTICS_WORKERS=${ANALYTICS_WORKERS}\nANALYTICS_MAX_CONNECTIONS=${ANALYTICS_MAX_CONNECTIONS}\nANALYTICS_RATE_LIMIT=${ANALYTICS_RATE_LIMIT}\n\n# Database Configuration\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=analytics\nPOSTGRES_USER=analytics\nPOSTGRES_PASSWORD=${POSTGRES_PASSWORD:-analytics_password}\n\n# Redis Configuration\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_DB=0\n\n# API Configuration\nAPI_HOST=0.0.0.0\nAPI_PORT=8080\nWEBSOCKET_PORT=8081\nJWT_SECRET=${JWT_SECRET:-$(openssl rand -hex 32)}\n\n# Monitoring Configuration\nMONITORING_ENABLED=true\nMETRICS_PORT=9090\nHEALTH_CHECK_PORT=8082\n\n# Storage Configuration\nDATA_RETENTION_DAYS=365\nARCHIVE_ENABLED=true\nARCHIVE_PATH=/data/archives\n\n# Security Configuration\nENCRYPTION_ENABLED=true\nAUDIT_LOGGING=true\nRATE_LIMITING=true\nEOF\n    \n    success \"Configuration deployment completed\"\n}\n\n# Start services\nstart_services() {\n    info \"Starting analytics services...\"\n    \n    # Start all services\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" up -d\n    \n    # Wait for services to be healthy\n    info \"Waiting for services to become healthy...\"\n    local services=(\"analytics-api\" \"ml-insights\" \"sla-monitor\" \"reporting-system\")\n    \n    for service in \"${services[@]}\"; do\n        info \"Waiting for $service to be healthy...\"\n        local retries=60\n        while [[ $retries -gt 0 ]]; do\n            if docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" ps | grep -q \"$service.*healthy\"; then\n                success \"$service is healthy\"\n                break\n            fi\n            sleep 5\n            ((retries--))\n        done\n        \n        if [[ $retries -eq 0 ]]; then\n            error \"$service failed to become healthy\"\n            docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" logs \"$service\"\n            exit 1\n        fi\n    done\n    \n    success \"All services started successfully\"\n}\n\n# Health checks\nrun_health_checks() {\n    info \"Running comprehensive health checks...\"\n    \n    local health_checks_passed=0\n    local total_health_checks=6\n    \n    # API health check\n    if curl -f \"http://localhost:8080/health\" &> /dev/null; then\n        success \"API health check passed\"\n        ((health_checks_passed++))\n    else\n        error \"API health check failed\"\n    fi\n    \n    # Database health check\n    if docker exec analytics_postgres pg_isready -U analytics &> /dev/null; then\n        success \"Database health check passed\"\n        ((health_checks_passed++))\n    else\n        error \"Database health check failed\"\n    fi\n    \n    # Redis health check\n    if docker exec analytics_redis redis-cli ping | grep -q PONG; then\n        success \"Redis health check passed\"\n        ((health_checks_passed++))\n    else\n        error \"Redis health check failed\"\n    fi\n    \n    # ML insights service check\n    if docker ps | grep -q \"ml-insights.*Up\"; then\n        success \"ML insights service check passed\"\n        ((health_checks_passed++))\n    else\n        error \"ML insights service check failed\"\n    fi\n    \n    # SLA monitor service check\n    if docker ps | grep -q \"sla-monitor.*Up\"; then\n        success \"SLA monitor service check passed\"\n        ((health_checks_passed++))\n    else\n        error \"SLA monitor service check failed\"\n    fi\n    \n    # Reporting system check\n    if docker ps | grep -q \"reporting-system.*Up\"; then\n        success \"Reporting system service check passed\"\n        ((health_checks_passed++))\n    else\n        error \"Reporting system service check failed\"\n    fi\n    \n    info \"Health checks: $health_checks_passed/$total_health_checks passed\"\n    \n    if [[ $health_checks_passed -eq $total_health_checks ]]; then\n        success \"All health checks passed\"\n    else\n        error \"Some health checks failed\"\n        exit 1\n    fi\n}\n\n# Setup monitoring\nsetup_monitoring() {\n    info \"Setting up monitoring and alerting...\"\n    \n    # Start monitoring stack\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.monitoring.yml\" up -d\n    \n    # Wait for Prometheus to be ready\n    local retries=30\n    while [[ $retries -gt 0 ]]; do\n        if curl -f \"http://localhost:9090/-/ready\" &> /dev/null; then\n            break\n        fi\n        sleep 2\n        ((retries--))\n    done\n    \n    if [[ $retries -eq 0 ]]; then\n        warn \"Prometheus failed to start within timeout\"\n    else\n        success \"Prometheus is ready\"\n    fi\n    \n    # Configure Grafana dashboards\n    info \"Configuring Grafana dashboards...\"\n    sleep 10  # Wait for Grafana to be ready\n    \n    # Import analytics dashboards\n    curl -X POST \"http://admin:admin@localhost:3000/api/dashboards/db\" \\\n        -H \"Content-Type: application/json\" \\\n        -d @\"$PROJECT_ROOT/monitoring/dashboards/analytics-overview.json\" || warn \"Failed to import analytics dashboard\"\n    \n    curl -X POST \"http://admin:admin@localhost:3000/api/dashboards/db\" \\\n        -H \"Content-Type: application/json\" \\\n        -d @\"$PROJECT_ROOT/monitoring/dashboards/sla-monitoring.json\" || warn \"Failed to import SLA dashboard\"\n    \n    success \"Monitoring setup completed\"\n}\n\n# Initialize sample data\ninitialize_sample_data() {\n    if [[ \"$ENV\" != \"production\" ]]; then\n        info \"Initializing sample data for $ENV environment...\"\n        \n        # Run sample data generation\n        python3 \"$PROJECT_ROOT/scripts/generate_sample_data.py\" --environment=\"$ENV\"\n        \n        success \"Sample data initialization completed\"\n    else\n        info \"Skipping sample data initialization in production\"\n    fi\n}\n\n# Post-deployment tasks\npost_deployment() {\n    info \"Running post-deployment tasks...\"\n    \n    # Create log rotation configuration\n    sudo tee /etc/logrotate.d/analytics << EOF > /dev/null\n$PROJECT_ROOT/logs/*.log {\n    daily\n    missingok\n    rotate 30\n    compress\n    delaycompress\n    notifempty\n    copytruncate\n    postrotate\n        docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" restart analytics-api || true\n    endscript\n}\nEOF\n    \n    # Set up systemd service for auto-start\n    if command -v systemctl &> /dev/null; then\n        info \"Creating systemd service...\"\n        sudo tee /etc/systemd/system/analytics.service << EOF > /dev/null\n[Unit]\nDescription=Production Analytics System\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nWorkingDirectory=$PROJECT_ROOT\nExecStart=/usr/local/bin/docker-compose -f docker-compose.yml up -d\nExecStop=/usr/local/bin/docker-compose -f docker-compose.yml down\nTimeoutStartSec=0\n\n[Install]\nWantedBy=multi-user.target\nEOF\n        \n        sudo systemctl daemon-reload\n        sudo systemctl enable analytics.service\n        success \"Systemd service created and enabled\"\n    fi\n    \n    # Create backup script\n    cat > \"$PROJECT_ROOT/scripts/backup.sh\" << 'EOF'\n#!/bin/bash\n\n# Analytics Backup Script\nBACKUP_DIR=\"/backup/analytics/$(date +%Y%m%d)\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup database\ndocker exec analytics_postgres pg_dump -U analytics analytics > \"$BACKUP_DIR/analytics_db.sql\"\n\n# Backup configuration\ncp -r \"$(dirname \"$0\")/../config\" \"$BACKUP_DIR/\"\n\n# Backup logs (last 7 days)\nfind \"$(dirname \"$0\")/../logs\" -name \"*.log\" -mtime -7 -exec cp {} \"$BACKUP_DIR/\" \\;\n\n# Compress backup\ntar -czf \"$BACKUP_DIR.tar.gz\" -C \"$(dirname \"$BACKUP_DIR\")\" \"$(basename \"$BACKUP_DIR\")\"\nrm -rf \"$BACKUP_DIR\"\n\necho \"Backup completed: $BACKUP_DIR.tar.gz\"\nEOF\n    \n    chmod +x \"$PROJECT_ROOT/scripts/backup.sh\"\n    \n    # Add backup to crontab\n    (crontab -l 2>/dev/null; echo \"0 2 * * * $PROJECT_ROOT/scripts/backup.sh\") | crontab -\n    \n    success \"Post-deployment tasks completed\"\n}\n\n# Deployment summary\nprint_deployment_summary() {\n    info \"===========================================\"\n    info \"   PRODUCTION ANALYTICS DEPLOYMENT SUMMARY\"\n    info \"===========================================\"\n    info \"\"\n    info \"Environment: $ENV\"\n    info \"Deployment Time: $(date)\"\n    info \"Project Root: $PROJECT_ROOT\"\n    info \"\"\n    info \"Services:\"\n    info \"  📊 Analytics API: http://localhost:8080\"\n    info \"  🤖 ML Insights Engine: Running\"\n    info \"  📋 SLA Monitor: Running\"\n    info \"  📄 Reporting System: Running\"\n    info \"\"\n    info \"Monitoring:\"\n    info \"  📈 Prometheus: http://localhost:9090\"\n    info \"  📊 Grafana: http://localhost:3000 (admin/admin)\"\n    info \"  🏥 Health Check: http://localhost:8082/health\"\n    info \"\"\n    info \"Documentation:\"\n    info \"  📚 API Docs: http://localhost:8080/docs\"\n    info \"  🔧 OpenAPI Spec: http://localhost:8080/openapi.json\"\n    info \"\"\n    info \"Configuration:\"\n    info \"  📁 Config Directory: $CONFIG_DIR\"\n    info \"  📝 Environment File: $PROJECT_ROOT/.env\"\n    info \"  📊 Data Retention: $PROJECT_ROOT/config/data-retention.toml\"\n    info \"\"\n    info \"Useful Commands:\"\n    info \"  View logs: docker-compose logs -f\"\n    info \"  Restart services: docker-compose restart\"\n    info \"  Stop services: docker-compose down\"\n    info \"  Backup data: $PROJECT_ROOT/scripts/backup.sh\"\n    info \"\"\n    success \"🎉 Analytics deployment completed successfully!\"\n    info \"===========================================\"\n}\n\n# Cleanup on failure\ncleanup_on_failure() {\n    error \"Deployment failed. Cleaning up...\"\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.yml\" down || true\n    docker-compose -f \"$PROJECT_ROOT/docker-compose.monitoring.yml\" down || true\n    exit 1\n}\n\n# Signal handlers\ntrap cleanup_on_failure ERR\ntrap 'info \"Deployment interrupted by user\"; exit 1' INT TERM\n\n# Main deployment flow\nmain() {\n    info \"🚀 Starting Production Analytics Deployment\"\n    info \"Environment: $ENV\"\n    info \"Project Root: $PROJECT_ROOT\"\n    info \"Log File: $LOG_FILE\"\n    \n    # Create necessary directories\n    mkdir -p \"$PROJECT_ROOT/logs\"\n    mkdir -p \"$PROJECT_ROOT/data\"\n    mkdir -p \"$PROJECT_ROOT/backups\"\n    \n    # Deployment steps\n    load_config\n    check_prerequisites\n    setup_database\n    build_services\n    deploy_configuration\n    start_services\n    run_health_checks\n    setup_monitoring\n    initialize_sample_data\n    post_deployment\n    print_deployment_summary\n    \n    # Save deployment metadata\n    cat > \"$PROJECT_ROOT/.deployment-info\" << EOF\nDEPLOYMENT_DATE=$(date -u +%Y-%m-%dT%H:%M:%SZ)\nDEPLOYMENT_ENV=$ENV\nDEPLOYMENT_VERSION=4.0.0\nDEPLOYMENT_LOG=$LOG_FILE\nEOF\n    \n    success \"✅ Production Analytics deployment completed successfully!\"\n}\n\n# Help function\nshow_help() {\n    cat << EOF\nProduction Analytics Deployment Script\n\nUsage: $0 [OPTIONS]\n\nOptions:\n    -e, --environment ENV    Set deployment environment (development, staging, production)\n    -c, --config FILE        Use custom configuration file\n    -h, --help              Show this help message\n    --skip-health-checks    Skip health checks (not recommended)\n    --skip-monitoring       Skip monitoring setup\n    --dry-run               Show what would be deployed without actually deploying\n\nEnvironment Variables:\n    ENVIRONMENT             Deployment environment (overrides -e option)\n    POSTGRES_PASSWORD       PostgreSQL password\n    JWT_SECRET              JWT secret for API authentication\n\nExamples:\n    $0                                      # Deploy with auto-detected environment\n    $0 -e production                        # Deploy to production\n    $0 -e staging --skip-monitoring         # Deploy to staging without monitoring\n    ENVIRONMENT=production $0               # Deploy to production using env var\n\nFor more information, see the deployment documentation.\nEOF\n}\n\n# Parse command line arguments\nSKIP_HEALTH_CHECKS=false\nSKIP_MONITORING=false\nDRY_RUN=false\n\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        -e|--environment)\n            ENV=\"$2\"\n            shift 2\n            ;;\n        -c|--config)\n            CONFIG_FILE=\"$2\"\n            shift 2\n            ;;\n        --skip-health-checks)\n            SKIP_HEALTH_CHECKS=true\n            shift\n            ;;\n        --skip-monitoring)\n            SKIP_MONITORING=true\n            shift\n            ;;\n        --dry-run)\n            DRY_RUN=true\n            shift\n            ;;\n        -h|--help)\n            show_help\n            exit 0\n            ;;\n        *)\n            error \"Unknown option: $1\"\n            show_help\n            exit 1\n            ;;\n    esac\ndone\n\n# Dry run mode\nif [[ \"$DRY_RUN\" == \"true\" ]]; then\n    info \"🔍 DRY RUN MODE - No actual deployment will be performed\"\n    info \"Would deploy to environment: $ENV\"\n    info \"Would use project root: $PROJECT_ROOT\"\n    info \"Would create log file: $LOG_FILE\"\n    exit 0\nfi\n\n# Run main deployment\nmain \"$@\"\n"