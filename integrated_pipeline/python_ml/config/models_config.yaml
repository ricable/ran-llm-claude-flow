# Enhanced Multi-Model Pipeline Configuration
# Optimized for M3 Max with intelligent model selection and resource management
# Weeks 2-4 Core Pipeline Implementation

# Model configurations for Qwen3 variants with advanced optimization
models:
  # Fast Processing Model - Optimized for throughput
  qwen3_1_7b:
    size: "1.7B"
    backend: "mlx"  # MLX for Apple Silicon optimization
    model_path: "Qwen/Qwen2.5-1.5B-Instruct"
    fallback_path: "microsoft/DialoGPT-medium"  # Fallback if Qwen unavailable
    
    # Memory and resource allocation
    memory_budget_gb: 4.0
    max_memory_gb: 6.0  # Emergency allocation
    memory_efficiency_target: 0.85
    
    # Quantization configuration
    quantization:
      enabled: true
      bits: 4
      group_size: 128
      optimization_level: "aggressive"  # More aggressive for speed
      cache_quantized: true
    
    # Generation parameters optimized for speed
    generation:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      batch_size: 8
      max_batch_size: 16
      streaming: true
      early_stopping: true
    
    # Performance targets
    performance_targets:
      max_inference_time_s: 2.0
      min_throughput_docs_hour: 35
      target_quality_score: 0.72
      memory_efficiency: 0.85
    
    # Specialized use cases
    use_cases:
      - "fast_processing"
      - "simple_qa"
      - "parameter_extraction"
      - "real_time_analysis"
      - "batch_processing"
    
    # Optimization strategies
    optimization:
      enable_kv_cache: true
      enable_mixed_precision: true
      enable_graph_optimization: true
      prefill_optimization: true
      decode_optimization: true
    
  # Balanced Processing Model - Quality/Speed tradeoff
  qwen3_7b:
    size: "7B"
    backend: "mlx"
    model_path: "Qwen/Qwen2.5-7B-Instruct"
    fallback_path: "microsoft/DialoGPT-large"
    
    # Memory configuration
    memory_budget_gb: 12.0
    max_memory_gb: 15.0
    memory_efficiency_target: 0.88
    
    # Quantization for balanced performance
    quantization:
      enabled: true
      bits: 4
      group_size: 128
      optimization_level: "balanced"
      cache_quantized: true
      dynamic_quantization: true
    
    # Generation parameters for quality balance
    generation:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      batch_size: 4
      max_batch_size: 8
      streaming: true
      repetition_penalty: 1.1
    
    # Performance targets
    performance_targets:
      max_inference_time_s: 5.0
      min_throughput_docs_hour: 25
      target_quality_score: 0.82
      memory_efficiency: 0.88
    
    # Use cases for balanced processing
    use_cases:
      - "balanced_processing"
      - "technical_qa"
      - "complex_reasoning"
      - "document_analysis"
      - "content_generation"
    
    # Advanced optimization
    optimization:
      enable_kv_cache: true
      enable_mixed_precision: true
      enable_attention_optimization: true
      enable_layer_fusion: true
      dynamic_batching: true
    
  # Quality Processing Model - Maximum accuracy
  qwen3_30b:
    size: "30B"
    backend: "lmstudio"  # LM Studio for large model stability
    model_path: "qwen2.5-32b-instruct"
    fallback_path: "qwen2.5-14b-instruct"  # Smaller fallback
    
    # Memory configuration for large model
    memory_budget_gb: 30.0
    max_memory_gb: 35.0
    memory_efficiency_target: 0.90
    
    # Quantization (handled by LM Studio)
    quantization:
      enabled: false  # LM Studio manages quantization
      external_quantization: true
      preferred_precision: "int4"
    
    # Generation parameters for maximum quality
    generation:
      max_tokens: 8192
      temperature: 0.7
      top_p: 0.9
      top_k: 100
      batch_size: 2
      max_batch_size: 4
      streaming: false  # Full generation for quality
      repetition_penalty: 1.05
      length_penalty: 1.0
    
    # Performance targets (quality focused)
    performance_targets:
      max_inference_time_s: 15.0
      min_throughput_docs_hour: 15
      target_quality_score: 0.91
      memory_efficiency: 0.90
    
    # High-quality use cases
    use_cases:
      - "high_quality"
      - "expert_analysis"
      - "complex_documentation"
      - "research_analysis"
      - "quality_assessment"
    
    # Quality-focused optimization
    optimization:
      enable_quality_boost: true
      enable_consistency_check: true
      multi_pass_generation: false
      quality_validation: true

# Advanced Memory Management for M3 Max (45GB total allocation)
memory:
  total_budget_gb: 45.0
  allocation:
    model_weights: 30.0    # 67% for model parameters
    working_memory: 10.0   # 22% for inference operations
    cache: 5.0            # 11% for embeddings and KV cache
  
  optimization:
    enable_unified_memory: true
    memory_pool_size: 1024  # MB
    garbage_collection_threshold: 0.9
    
# Model selection strategy
selection:
  strategy: "adaptive"  # adaptive, performance, quality, memory
  
  # Complexity thresholds for automatic selection
  complexity_thresholds:
    simple: 0.3      # Use 1.5B model
    moderate: 0.7    # Use 7B model
    complex: 1.0     # Use 32B model
    
  # Quality requirements mapping
  quality_mapping:
    basic: "qwen3_1_5b"
    standard: "qwen3_7b"
    premium: "qwen3_32b"
    
  # Fallback chain
  fallback_order:
    - "qwen3_7b"     # Primary fallback
    - "qwen3_1_5b"   # Secondary fallback
    - "qwen3_32b"    # Last resort (if available)

# External service configurations
services:
  lmstudio:
    base_url: "http://localhost:1234"
    api_version: "v1"
    timeout: 120
    max_retries: 3
    connection_pool_size: 8
    health_check_interval: 30
    
  ollama:
    base_url: "http://localhost:11434"
    timeout: 60
    max_retries: 2
    stream_enabled: false
    
# Performance optimization settings
performance:
  # MLX specific settings
  mlx:
    enable_metal_acceleration: true
    memory_fraction: 0.85
    enable_fp16: true
    enable_quantization: true
    batch_optimization: true
    
  # Batch processing
  batching:
    max_batch_size: 8
    max_wait_time_ms: 100
    dynamic_batching: true
    priority_scheduling: true
    
  # Caching
  caching:
    enable_model_cache: true
    enable_embedding_cache: true
    cache_size_mb: 1024
    ttl_seconds: 3600
    
  # Threading
  threading:
    max_workers: 8
    enable_async_processing: true
    queue_size: 1000

# Quality assessment configuration
quality:
  thresholds:
    minimum: 0.6      # Reject below this
    target: 0.75      # Aim for this
    excellent: 0.9    # Flag as high quality
    
  metrics:
    weights:
      relevance: 0.25
      accuracy: 0.25
      diversity: 0.15
      technical_density: 0.15
      completeness: 0.10
      coherence: 0.10
      
  assessment:
    enable_semantic_similarity: true
    enable_technical_validation: true
    diversity_threshold: 0.8
    
# Semantic processing configuration
semantic:
  # Sentence transformer model
  embedding_model: "all-MiniLM-L6-v2"
  embedding_device: "mps"  # mps for Apple Silicon, cuda for NVIDIA, cpu for fallback
  
  # Text processing
  max_sequence_length: 8192
  chunk_size: 1024
  chunk_overlap: 128
  
  # Technical term extraction
  technical_terms:
    enable_acronym_detection: true
    enable_parameter_extraction: true
    min_term_frequency: 2
    
# Document analysis settings
document_analysis:
  complexity_factors:
    weights:
      length: 0.2
      technical_density: 0.3
      parameter_count: 0.3
      sentence_complexity: 0.2
      
  structure_analysis:
    enable_section_detection: true
    enable_table_detection: true
    enable_code_detection: true
    
# Question generation settings
qa_generation:
  question_types:
    definition: 0.25
    parameter: 0.30
    process: 0.20
    configuration: 0.15
    troubleshooting: 0.10
    
  difficulty_distribution:
    basic: 0.30
    intermediate: 0.40
    advanced: 0.25
    expert: 0.05
    
  diversity:
    min_similarity_threshold: 0.3
    max_attempts: 10
    enable_paraphrasing: true

# Logging and monitoring
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "structured"  # structured, simple
  
  loggers:
    model_manager: "INFO"
    semantic_processor: "INFO"
    mlx_accelerator: "INFO"
    ipc_client: "INFO"
    
  performance_logging:
    enable_metrics: true
    log_interval: 60  # seconds
    detailed_timing: false

# Development and debugging
development:
  debug_mode: false
  enable_profiling: false
  save_intermediate_results: false
  
  testing:
    mock_external_services: false
    use_sample_data: false
    enable_benchmarking: true
    
# Integration settings
integration:
  # IPC configuration
  ipc:
    pipe_path: "/tmp/rust_python_ipc"
    shared_memory_size_mb: 1024
    timeout_seconds: 30
    heartbeat_interval: 10
    
  # Performance targets
  targets:
    documents_per_hour: 25
    qa_pairs_per_document: 5
    quality_threshold: 0.75
    memory_efficiency: 0.9
    
  # Error handling
  error_handling:
    max_retries: 3
    retry_delay: 1.0
    enable_fallback_models: true
    timeout_escalation: true
